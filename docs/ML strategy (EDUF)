Choice of ML algoritm: Multi-Layer Perceptron

Key mechanisms (function in model): forward propagation, loss function, backpropagation and optimization

From the website: https://www.geeksforgeeks.org/deep-learning/multi-layer-perceptron-learning-in-tensorflow/ 
---------------------------------------------------------------------------------------------------------------
Basic working principle of Multi-Layer Perceptron:
  1. Forward Propagation
  1.1 Weighted Sum: The input (X) is multiplied by its weight and concatenated by its weight respectively.
  1.2 Activation Function: The output is introduced to a non-linearity (such as: sigmoid, ReLU or Tanh).

  2. Loss Function
    Once the network generates an output the next step is to calculate the loss using a loss function. 
    In supervised learning this compares the predicted output (Y_pred) to the actual label (Y).
    For a classification problem the commonly used loss function is binary cross-entropy (i.e. Negative Log-likelihood).
    For regression problesm the mean squared error (MSE) is often used.

  3. Backpropagation
    The goal of training an MLP is to minimize the loss function by adjusting the network's weights and biases.
    This is achieved through backpropagation:
  3.1 Gradient Calculation: The gradients of the loss function with respect to each weight and bias are calculated using the chain rule of calculus.
  3.2 Error Propagation: The error is propagated back through the network, layer by layer.
  3.3 Gradient Descent: The network updates the weights and biases by moving in the opposite direction of the gradient to reduce the loss.

  4. Optimization
    MLPs rely on optimization algorithms to iteratively refine the weights and biases during training. 
  	Popular optimization methods include:
    - Stochastic Gradient Descent (SGD): Updates the weights based on a single sample or a small batch of data
    - Adam Optimizer: An extension of SGD that incorporates momentum and adaptive learning rates for more efficient training
---------------------------------------------------------------------------------------------------------------

From the website: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips
---------------------------------------------------------------------------------------------------------------
Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data.
For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1.
Note that you must apply the same scaling to the test set for meaningful results.

Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. For relatively large datasets, however, Adam is very robust.
It usually converges quickly and gives pretty good performance.
SGD with momentum or nesterovâ€™s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.
---------------------------------------------------------------------------------------------------------------

How do we choose the right amount of hidden layers and neurons?
From the website: https://stackoverflow.com/questions/10565868/multi-layer-perceptron-mlp-architecture-criteria-for-choosing-number-of-hidde#:~:text=RoTs%20based%20on%20size%20of,of%20%22incorrect%22%20network%20architecture.
---------------------------------------------------------------------------------------------------------------
Rule of thumb for amount of hidden layers: 
  Always start with one hidden layer. 
  Many difficult classification/regression problems are solved using single-hidden-layer MLPs.
  Only if the model is really underfitting the data, then you can increase the amount of hidden layers gradually.
Rule of thumb for amount of nodes in the hidden layers:
---------------------------------------------------------------------------------------------------------------
