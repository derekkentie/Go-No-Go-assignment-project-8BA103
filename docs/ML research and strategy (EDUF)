Choice of ML algoritm: Multi-Layer Perceptron
Key mechanisms (function in model): forward propagation, loss function, backpropagation and optimization



From the website: https://www.geeksforgeeks.org/deep-learning/multi-layer-perceptron-learning-in-tensorflow/ 
---------------------------------------------------------------------------------------------------------------
Basic working principle of Multi-Layer Perceptron:
  1. Forward Propagation
  1.1 Weighted Sum: The input (X) is multiplied by its weight and concatenated by its weight respectively.
  1.2 Activation Function: The output is introduced to a non-linearity (such as: sigmoid, ReLU or Tanh).

  2. Loss Function
    Once the network generates an output the next step is to calculate the loss using a loss function. 
    In supervised learning this compares the predicted output (Y_pred) to the actual label (Y).
    For a classification problem the commonly used loss function is binary cross-entropy (i.e. Negative Log-likelihood).
    For regression problesm the mean squared error (MSE) is often used.

  3. Backpropagation
    The goal of training an MLP is to minimize the loss function by adjusting the network's weights and biases.
    This is achieved through backpropagation:
  3.1 Gradient Calculation: The gradients of the loss function with respect to each weight and bias are calculated using the chain rule of calculus.
  3.2 Error Propagation: The error is propagated back through the network, layer by layer.
  3.3 Gradient Descent: The network updates the weights and biases by moving in the opposite direction of the gradient to reduce the loss.

  4. Optimization
    MLPs rely on optimization algorithms to iteratively refine the weights and biases during training. 
  	Popular optimization methods include:
    - Stochastic Gradient Descent (SGD): Updates the weights based on a single sample or a small batch of data
    - Adam Optimizer: An extension of SGD that incorporates momentum and adaptive learning rates for more efficient training
---------------------------------------------------------------------------------------------------------------



From the website: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips
---------------------------------------------------------------------------------------------------------------
Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data.
For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1.
Note that you must apply the same scaling to the test set for meaningful results.

Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. For relatively large datasets, however, Adam is very robust.
It usually converges quickly and gives pretty good performance.
SGD with momentum or nesterovâ€™s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.
---------------------------------------------------------------------------------------------------------------



How do we choose the right amount of hidden layers and neurons?
From the website: https://stackoverflow.com/questions/10565868/multi-layer-perceptron-mlp-architecture-criteria-for-choosing-number-of-hidde#:~:text=RoTs%20based%20on%20size%20of,of%20%22incorrect%22%20network%20architecture.
---------------------------------------------------------------------------------------------------------------
Rule of thumb for amount of hidden layers: 
  Always start with one hidden layer. 
  Many difficult classification/regression problems are solved using single-hidden-layer MLPs.
  Only if the model is really underfitting the data, then you can increase the amount of hidden layers gradually.
Rule of thumb for amount of nodes in the hidden layers:
  The size has to be somewhere between the input layer and output layer size
  We can also calculate an estimated number of hidden nodes with: (Number of inputs + outputs) x 2/3
RoT based on principal components:
  Typically, we specify as many hidden nodes as dimensions [principal components] needed to capture 70-90% of the variance of the input data set.

[Advice from how the athor of the article works in practice]
INPUT LAYER: the size of my data vactor (the number of features in my model) + 1 for the bias node and not including the response variable, of course

OUTPUT LAYER: solely determined by my model: regression (one node) versus classification (number of nodes equivalent to the number of classes, assuming softmax)

HIDDEN LAYER: to start, one hidden layer with a number of nodes equal to the size of the input layer. 
The "ideal" size is more likely to be smaller (i.e, some number of nodes between the number in the input layer and the number in the output layer) rather than larger--again, this is just an empirical observation, and the bulk of this observation is my own experience. 
If the project justified the additional time required, then I start with a single hidden layer comprised of a small number of nodes, then (as i explained just above) I add nodes to the Hidden Layer, one at a time, while calculating the generalization error, training error, bias, and variance. 
When generalization error has dipped and just before it begins to increase again, the number of nodes at that point is my choice.
---------------------------------------------------------------------------------------------------------------


What kind of non-linearity function do we use?
From the websites: https://www.geeksforgeeks.org/deep-learning/softmax-vs-sigmoid-activation-function/ and https://www.geeksforgeeks.org/machine-learning/activation-functions-neural-networks/ 
---------------------------------------------------------------------------------------------------------------
The sigmoid function is used for binary class problems, the softmax function on the other hand is used for multi class problems.
Other functions like ReLU and Tanh are also used for binary class problems, but they are not suited for multi class problems:
"Activation functions like Softmax allow the model to handle complex multi-class problems, whereas simpler functions like ReLU or Leaky ReLU are used for basic layers.".
Our training dataset shows to three different classes (bad, medium, good), so we need to implement a softmax as activation function
---------------------------------------------------------------------------------------------------------------


Advice from a friend of mine: use classes in your model to save the forward and backwardpropagation to improve your model

What I need for my model:
- Data extraction function
- linear function
- activation function
- training function
- prediction function
- loss function
